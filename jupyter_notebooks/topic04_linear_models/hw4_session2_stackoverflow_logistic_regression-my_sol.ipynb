{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия № 2\n",
    "Авторы материала: Павел Нестеров. Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашняя работа №4\n",
    "## <center> Логистическая регрессия в задаче тегирования вопросов StackOverflow\n",
    "\n",
    "**Надо вывести формулы, где это просится (да, ручка и бумажка), заполнить код в клетках и выбрать ответы в [веб-форме](https://docs.google.com/forms/d/1I_ticU8rpeoGJjsBUcaInpvgdxdq60hV7IcSvo4rlGo/).**\n",
    "\n",
    "## 0. Описание задачи\n",
    "\n",
    "В этой домашней работе мы с вами изучим и запрограммируем модель для прогнозирования тегов по тексту вопроса на базе многоклассовой логистической регрессии. В отличие от обычной постановки задачи классификации (multiclass), в данном случае один пример может принадлежать одновременно к нескольким классам (multilabel). Мы будем реализовывать онлайн версию алгоритма мультиклассовой классификации.\n",
    "\n",
    "Мы будем использовать небольшую выборку из протеггированных вопросов с сайта StackOverflow размером в 125 тысяч примеров (около 150 Мб, скачайте по [этой](https://drive.google.com/open?id=0B4bl7YMqDnViYVo0V2FubFVhMFE) ссылке).\n",
    "\n",
    "PS: Можно показать, что такая реализация совсем не эффективная и проще было бы использовать векторизированные вычисления. Для данного датасета так и есть. Но на самом деле подобные реализации используются в жизни, но естественно, написаны они не на Python. Например, в онлайн моделях прогнозирования [CTR](https://en.wikipedia.org/wiki/Click-through_rate) юзеру показывается баннер, затем в зависимости от наличия клика происходит обновление параметров модели. В реальной жизни параметров модели может быть несколько сотен миллионов, а у юзера из этих ста миллионов от силы сто или тысяча параметров отличны от нуля, векторизировать такие вычисления не очень эффективно. Обычно все это хранится в огромных кластерах в in-memory базах данных, а обработка пользователей происходит распределенно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# поменяйте на свой путь\n",
    "DS_FILE_NAME = '../../data/stackoverflow_sample_125k.tsv'\n",
    "TAGS_FILE_NAME = '../../data/top10_tags.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'javascript', 'java', 'android', 'c++', 'ios', 'html', 'php', 'c#', 'python', 'jquery'}\n"
     ]
    }
   ],
   "source": [
    "top_tags = []\n",
    "with open(TAGS_FILE_NAME, 'r') as f:\n",
    "    for line in f:\n",
    "        top_tags.append(line.strip())\n",
    "top_tags = set(top_tags)\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Многоклассовая логистическая регрессия\n",
    "\n",
    "Вспомним, как получается логистическая регрессия для двух классов $\\left\\{0, 1\\right\\}$, вероятность принадлежности объекта к классу $1$ выписывается по теореме Байеса:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} \\\\\n",
    "&=& \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\vec{x}$ – вектор признаков объекта\n",
    "- $\\sigma$ – обозначение функции логистического сигмоида при скалярном аргументе\n",
    "- $a = \\log \\frac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^M w_i x^i$ – это отношение мы моделируем линейной функцией от признаков объекта и параметров модели\n",
    "\n",
    "Данное выражение легко обобщить до множества из $K$ классов, изменится только знаменатель в формуле Байеса. Запишем вероятность принадлежности объекта к классу $k$:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\vec{x} \\mid c = i\\right)p\\left(c = i\\right)} \\\\\n",
    "&=& \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} \\\\\n",
    "&=& \\sigma_k\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\sigma_k$ – обозначение функции softmax при векторном аргументе\n",
    "- $z_k = \\log p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^M w_{ki} x^i$ – это выражение моделируется линейной функций от признаков объекта и параметров класса $k$\n",
    "\n",
    "Для моделирования полного правдоподобия примера мы используем [категориальное распределение](https://en.wikipedia.org/wiki/Categorical_distribution), а лучше его логарифм (для удобства):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\mathcal{L} = \\log p\\left({\\vec{x}}\\right) &=& \\log \\prod_{i=1}^K \\sigma_i\\left(\\vec{z}\\right)^{y_i} \\\\\n",
    "&=& \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Получается хорошо знакомая нам функция [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) (если домножить на $-1$). Правдоподобие нужно максимизировать, а, соответственно, перекрестную энтропию нужно минимизировать. Продифференцировав по параметрам модели, мы _легко_ получим правила обновления весов для градиентного спуска, **проделайте этот вывод, если вы его не делали** (если вы вдруг сдались, то на [этом](https://www.youtube.com/watch?v=-WiR16raQf4) видео есть разбор вывода, понимание этого вам понадобится для дальнейшего выполнения задания):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\vec{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "В стандартной формулировке получается, что вектор $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ образует дискретное вероятностное распределение, т.е. $\\sum_{i=1}^K \\sigma_i = 1$. Но в нашей постановке задачи каждый пример может иметь несколько тегов или одновременно принадлежать к нескольким классам. Для этого мы немного изменим модель:\n",
    "- будем считать, что все теги независимы друг от друга, т.е. каждый исход – это логистическая регрессия на два класса (либо есть тег, либо его нет), тогда вероятность наличия тега у примера запишется следующим образом (каждый тег/класс как и в многоклассовой логрегрессии имеет свой набор параметров):\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\vec{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=1}^M w_{ki} x^i \\right)$$\n",
    "- наличие каждого тега мы будем моделировать с помощью <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">распределения Бернулли</a>\n",
    "\n",
    "Ваше первое задание –  записать упрощенное выражение логарифма правдоподобия примера с признаками $\\vec{x}$. Как правило, многие алгоритмы оптимизации имеют интерфейс для минимизации функции, мы последуем этой же традиции, и домножим полученное выражение на $-1$, а во второй части выведем формулы для минимизации полученного выражения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. $\\large -\\mathcal{L} = -\\sum_{i=1}^M y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "2. $\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$ ответ\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^M z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вывод формулы обновления весов\n",
    "\n",
    "В качестве второго задания вам предоставляется возможность вывести формулу градиента для $-\\mathcal{L}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "2. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)$ ответ\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Имплементация базовой модели\n",
    "\n",
    "Вам предлагается каркас класса модели, разберите его внимательно, обращайте внимание на комментарии. Затем заполните пропуски, запустите полученную модель и ответьте на проверочный вопрос.\n",
    "\n",
    "Как вы могли уже заметить, при обновлении веса $w_{km}$ используется значение признака $x_m$, который равен $0$ если слова с индексом $m$ нет в предложении, и больше нуля, если такое слово есть. Соответственно, при вычислении линейной комбинации $z$ весов модели и признаков примера необходимо учитывать только ненулевые признаки объекта.\n",
    "\n",
    "Подсказка:\n",
    "- если реализовывать вычисление сигмоида так же, как в формуле, то при большом отрицательном значении $z$ вычисление $e^{-z}$ превратится в очень большое число, которое вылетит за допустимые пределы\n",
    "- в то же время $e^{-z}$ от большого положительного $z$ будет нулем\n",
    "- воспользуйтесь свойствами функции $\\sigma$ для того, чтобы пофиксить эту ошибку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags_top : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "        \n",
    "        self._y_pred = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "    def compute_jaccard_index(set_1, set_2):\n",
    "        n = len(set_1.intersection(set_2))\n",
    "        return n / float(len(set_1) + len(set_2) - n) \n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        prec = []\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "            \n",
    "                tags_pred = set()\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = 0\n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._b[tag] + self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                \n",
    "                    #sigma = 1/(1 + math.pow(math.exp(z),-1))\n",
    "                    if z > 100:\n",
    "                        sigma = 1\n",
    "                    else:\n",
    "                        sigma = np.exp(z)/(1 + np.exp(z))\n",
    "                \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    if sigma < tolerance:\n",
    "                        sigma = tolerance\n",
    "                    elif 1-sigma < tolerance:\n",
    "                        sigma = 1 - tolerance\n",
    "                        \n",
    "                    #задание 5\n",
    "                    if n >= top_n_train:\n",
    "                        self._y_pred[tag][n] = int(sigma>0.9)\n",
    "                        #if sigma>0.9:\n",
    "                            #tags_pred.add(tag)\n",
    "                    \n",
    "                    \n",
    "                    sample_loss += -(y*np.log(sigma) + (1 - y)*np.log(1 - sigma))\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma)\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                \n",
    "                #if n >= top_n_train:\n",
    "                    #prec.append(compute_jaccard_index(tags, tags_pred))\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "                \n",
    "       # return np.mean(prec)\n",
    "                \n",
    "                \n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62cc728ce9c64e6ebcf243b24d4d0e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим эксемпляр модели и пройдемся по датасету\n",
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, действительно ли значение отрицательного логарифмического правдоподобия уменьшалось. Так как мы используем стохастический градентный спуск, не стоит ожидать плавного падения функции ошибки. Мы воспользуемся скользящим средним с окном в 10 000 примеров, чтобы хоть как то сгладить график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA50AAAKrCAYAAAByeOlOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCcAAACZAyiPyaxY/jHuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f405e62f748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the loss function on the last 10k train samples: 29.40\n"
     ]
    }
   ],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите среднее значение функции стоимости на последних 10 000 примеров тренировочного набора, к какому из значений ваш ответ ближе всего?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 17.54\n",
    "2. 18.64\n",
    "3. 19.74\n",
    "4. 20.84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'android',\n",
       " 'c#',\n",
       " 'c++',\n",
       " 'html',\n",
       " 'ios',\n",
       " 'java',\n",
       " 'javascript',\n",
       " 'jquery',\n",
       " 'php',\n",
       " 'python'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Тестирование модели\n",
    "\n",
    "В базовой модели первые 100 000 строк используются для обучения, а оставшиеся – для тестирования. Как вы можете заметить, значение отрицательного логарифмического правдоподобия не очень информативно, хоть и позволяет сравнивать разные модели. В качестве четвертого задания вам необходимо модифицировать базовую модель таким образом, чтобы метод `iterate_file` возвращал значение _точности_ на тестовой части набора данных. \n",
    "\n",
    "Точность определим следующим образом:\n",
    "- считаем, что тег у вопроса присутствует, если спрогнозированная вероятность тега больше 0.9\n",
    "- точность одного примера расчитывается как [коэффициент Жаккара](https://ru.wikipedia.org/wiki/Коэффициент_Жаккара) между множеством настоящих тегов и предсказанных моделью\n",
    "  - например, если у примера настоящие теги ['html', 'jquery'], а по версии модели ['ios', 'html', 'java'], то коэффициент Жаккара будет равен |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- метод `iterate_file` возвращает **среднюю** точность на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags_top : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "        \n",
    "        self._y_pred = dict([(t, defaultdict(int)) for t in tags])\n",
    "        self._sum_ind = 0\n",
    "        self._count = 0\n",
    "        \n",
    "        \n",
    "    def compute_jaccard_index(set_1, set_2):\n",
    "        n = len(set_1.intersection(set_2))\n",
    "        return n / float(len(set_1) + len(set_2) - n) \n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        prec = []\n",
    "        sum_ind = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "            \n",
    "                tags_pred = set()\n",
    "                \n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = 0\n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._b[tag] + self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                \n",
    "                    #sigma = 1/(1 + math.pow(math.exp(z),-1))\n",
    "                    if z > 100:\n",
    "                        sigma = 1\n",
    "                    else:\n",
    "                        sigma = np.exp(z)/(1 + np.exp(z))\n",
    "                \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    if sigma < tolerance:\n",
    "                        sigma = tolerance\n",
    "                    elif 1-sigma < tolerance:\n",
    "                        sigma = 1 - tolerance\n",
    "                        \n",
    "                    #задание 5\n",
    "                    if n >= top_n_train:\n",
    "                        self._y_pred[tag][n] = int(sigma>0.9)\n",
    "                        if sigma>0.9:\n",
    "                            tags_pred.add(tag)\n",
    "                    \n",
    "                    \n",
    "                    sample_loss += -(y*np.log(sigma) + (1 - y)*np.log(1 - sigma))\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma)\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                \n",
    "                if n >= top_n_train:\n",
    "                    self._sum_ind += self.compute_jaccard_index(tags, tags_pred)\n",
    "                    self._count += 1\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "                \n",
    "        return float(self._sum_ind)/float(self._count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988b4662a2d44e8a81e61fdc442659bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "# выведем полученное значение с точностью до двух знаков\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 0.39\n",
    "2. 0.49\n",
    "3. 0.59\n",
    "4. 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. $L_2$-регуляризация\n",
    "\n",
    "В качестве пятого задания вам необходимо добавить в класс `LogRegressor` поддержку $L_2$-регуляризации. В методе `iterate_file` должен появиться параметр `lmbda=0.01` со значением по умолчанию. С учетом регуляризации новая функция стоимости примет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "Градиент первого члена суммы мы уже вывели, а для второго он имеет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "Если мы на каждом примере будем делать честное обновление всех весов, то все очень замедлится, ведь нам придется на каждой итерации пробегать по всем словам словаря. В ущерб теоретической точности вы используем грязный трюк: мы будем регуляризаровать только те слова, которые присутствуют в текущем предложении. Не забывайте, что смещение не регуляризируется. `sample_loss` тоже должен остаться без изменений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags_top : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "        \n",
    "        self._y_pred = dict([(t, defaultdict(int)) for t in tags])\n",
    "        self._sum_ind = 0\n",
    "        self._count = 0\n",
    "        \n",
    "        \n",
    "    def compute_jaccard_index(set_1, set_2):\n",
    "        n = len(set_1.intersection(set_2))\n",
    "        return n / float(len(set_1) + len(set_2) - n) \n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16,\n",
    "                     lmbda=0.01):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        prec = []\n",
    "        sum_ind = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "            \n",
    "                tags_pred = set()\n",
    "                \n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = 0\n",
    "                    sum_w2 = 0\n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._b[tag] + self._w[tag][self._vocab[word]]\n",
    "                        sum_w2 += self._w[tag][self._vocab[word]] ** 2\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                \n",
    "                    #sigma = 1/(1 + math.pow(math.exp(z),-1))\n",
    "                    if z > 100:\n",
    "                        sigma = 1\n",
    "                    else:\n",
    "                        sigma = np.exp(z)/(1 + np.exp(z))\n",
    "                \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    if sigma < tolerance:\n",
    "                        sigma = tolerance\n",
    "                    elif 1-sigma < tolerance:\n",
    "                        sigma = 1 - tolerance\n",
    "                        \n",
    "                    #задание 5\n",
    "                    if n >= top_n_train:\n",
    "                        self._y_pred[tag][n] = int(sigma>0.9)\n",
    "                        if sigma>0.9:\n",
    "                            tags_pred.add(tag)\n",
    "                    \n",
    "                    \n",
    "                    sample_loss += (-(y*np.log(sigma) + (1 - y)*np.log(1 - sigma)) + sum_w2)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        \n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:\n",
    "                            dLdw = (y - sigma) - lmbda*self._w[tag][self._vocab[word]]\n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                \n",
    "                if n >= top_n_train:\n",
    "                    self._sum_ind += self.compute_jaccard_index(tags, tags_pred)\n",
    "                    self._count += 1\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "                \n",
    "        return float(self._sum_ind)/float(self._count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопрос, к какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 0.3\n",
    "2. 0.35\n",
    "3. 0.4\n",
    "4. 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet регуляризация, вывод\n",
    "Помимо $L_2$ регуляризации, часто используется $L_1$ регуляризация.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "Если линейно объединить $L_1$ и $L_2$ регуляризацию, то полученный тип регуляризации называется ElasticNet:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- где $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "В качестве шестого вопроса вам предлагается вывести формулу градиента ElasticNet регуляризации (не учитывая $-\\mathcal{L}$). \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ElasticNet регуляризация, имплементация\n",
    "\n",
    "В качестве седьмой задачи, вам предлается изменить класс `LogRegressor` таким образом, чтобы метод `iterate_file` принимал два параметра со значениями по умолчанию `lmbda=0.0002` и `gamma=0.1`. Сделайте один проход по датасету с включенной ElasticNet регуляризацией и заданными значениями по умолчанию и ответьте на вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.59\n",
    "2. 0.69\n",
    "3. 0.79\n",
    "4. 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Самые важные слова для тега\n",
    "\n",
    "Прелесть линейных моделей в том, что они легко интерпретируемы. Вам предлагается вычислить, какие слова вносят наибольший вклад в вероятность каждого из тегов. А затем ответьте на контрольный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для многих тегов наличие самого тега в предложении является важным сигналом, у многих сам тег является самым сильным сигналом, что неудивительно. Для каких из тегов само название тега не входит в топ-5 самых важных?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. java, c#\n",
    "2. php, javascript\n",
    "3. html, jquery\n",
    "4. ios, android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Сокращаем размер словаря\n",
    "Сейчас количество слов в словаре около 90 000, если бы это была выборка из 10 миллионов вопросов с сайта StackOverflow, то размер словаря был бы миллионов 10. Регуляризировать модель можно не только изящно математически, но и топорно, например, ограничить размер словаря. Вам предоставляется возможность внести следующие изменения в класс `LogRegressor`:\n",
    "- добавить в метод `iterate_file` еще один аргумент со значением по умолчанию `update_vocab=True`\n",
    "- при `update_vocab=True` разрешать добавлять слова в словарь в режиме обучения\n",
    "- при `update_vocab=False` игнорировать слова не из словаря\n",
    "- добавить в класс метод `filter_vocab(n=10000)`, который оставит в словаре только топ-n самых популярных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# оставим только топ 10 000 слов\n",
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сделаем еще одну итерацию по датасету, уменьшив скорость обучения в 10 раз\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.48\n",
    "2. 0.58\n",
    "3. 0.68\n",
    "4. 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Прогнозирование тегов для новых вопросов\n",
    "\n",
    "В завершение сегодняшней домашки, вам предлагается реализовать метод `predict_proba`, который принимает строку,  содержащую вопрос, а возвращает список предсказанных тегов вопроса с их вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "model.filter_vocab(n=10000)\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопрос, какой или какие теги ассоциируются с данным вопросом, если порог принятия равен $0.9$?:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. android\n",
    "2. ios\n",
    "3. ios, php\n",
    "4. c#, c++, ods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
